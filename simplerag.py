# -*- coding: utf-8 -*-
"""SimpleRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P7Uq4CUpMHkvtvRg7M-Tcv5JUSD8fQKg
"""

!pip install transformers datasets sentence-transformers faiss-cpu torch -q

!pip install --upgrade datasets huggingface-hub

from datasets import load_dataset, Dataset
from sentence_transformers import SentenceTransformer
import torch
from transformers import pipeline

dataset = load_dataset("squad", split="train")

knowledge_base = dataset.select(range(1000)).to_pandas()
knowledge_base = knowledge_base.drop_duplicates(subset=["context"]).reset_index(drop=True)

knowledge_dataset = Dataset.from_pandas(knowledge_base)

print(knowledge_dataset)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

def generate_embeddings(batch):
    embeddings = embedding_model.encode(batch['context'])
    return {'embeddings': embeddings}

knowledge_dataset = knowledge_dataset.map(generate_embeddings, batched=True, batch_size=32)

print(knowledge_dataset)

knowledge_dataset.add_faiss_index(column='embeddings')

generator = pipeline('text-generation', model='distilgpt2', device=device)

def answer_question(query: str, k:int = 3):
    #1. Retrieve
    query_embedding = embedding_model.encode(query)
    scores, retrieved_examples = knowledge_dataset.get_nearest_examples('embeddings', query_embedding, k=k)

    #2. Generate
    context = "\n".join(retrieved_examples['context'])
    prompt = f"Answer the following question. If the answer is not in the context, say 'I dont know':\n{query}\nContext:\n{context}"
    answer = generator(prompt, max_length=150, num_return_sequences=1)
    return answer[0]['generated_text']

question = "What is the mascot of the University of Notre Dame?"
answer = answer_question(question)

print(f"Question: {question}")
print(f"Answer:\n{answer}")

